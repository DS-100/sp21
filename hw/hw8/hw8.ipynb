{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw8.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 8: Fairness in Housing Appraisal and Gradient Descent\n",
    "\n",
    "## Due Date:  Thursday Apr 8, 11:59 PM\n",
    "\n",
    "## This Assignment\n",
    "\n",
    "This assignment consists of 2 parts. In these parts, we will:\n",
    "\n",
    "1. Examine some of the HCE concerns from the housing dataset in the last assignment. \n",
    "1. Implement gradient descent, and show how it can be used to minimize differentiable functions, even including loss functions for non-linear models.\n",
    "\n",
    "\n",
    "Note that the first part of this assignment will use bold notation to represent vectors, i.e. $\\mathbf{x}$.\n",
    "\n",
    "## Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about the assignment, we ask that you **write your solutions individually**. If you do discuss the assignments with others please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point Distribution\n",
    "| Question | Points | \n",
    "|----------|--------|\n",
    "| 1 | 1 |\n",
    "| 2 | 1 |\n",
    "| 3 | 1 |\n",
    "| 4 | 1 |\n",
    "| 5 | 1 |\n",
    "| 6 | 1 |\n",
    "| 7 | 1 |\n",
    "| 8 | 1 |\n",
    "| 9a | 2 |\n",
    "| 9b | 2 |\n",
    "| 10a | 2 |\n",
    "| 10b | 2 |\n",
    "| 10c | 2 |\n",
    "| 10d | 2 |\n",
    "| Total | 20 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Human Contexts and Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Homework 7 we used data from the Cook County Assessor's Office to predict housing prices based on various features. From that assignment:\n",
    "\n",
    ">  The dataset you’ll be working with comes from the Cook County Assessor’s Office (CCAO) in Illinois, a government institution that determines property taxes across most of Chicago’s metropolitan area and its nearby suburbs. In the United States, all property owners are required to pay property taxes, which are then used to fund public services including education, road maintenance, and sanitation. These property tax assessments are based on property values estimated using statistical models that consider multiple factors, such as real estate value and construction cost.\n",
    "<br><br>\n",
    "The CCAO dataset consists of over 500 thousand records describing houses sold in Cook County in recent years (new records are still coming in every week!). The data set we will be working with has 61 features in total. An explanation of each variable can be found in the included `codebook.txt` file. Some of the columns have been filtered out to ensure this assignment doesn't become overly long when dealing with data cleaning and formatting.\n",
    "\n",
    "For reference, you can find the codebook [here](https://raw.githubusercontent.com/DS-100/sp21/main/hw/hw7/codebook.txt).\n",
    "\n",
    "The reason that we have access to such a well-documented data set and model is largely due to an investigation in 2017 that found the property taxes were levied in a manner that systematically benefited the wealthy. Use the context behind this data as described in [Lecture 17](https://ds100.org/sp21/lecture/lec17/) to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 1\n",
    "\n",
    "In the context of estimating the value of houses, what does error mean for an individual homeowner? How does it affect them in terms of property taxes?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 2\n",
    "\n",
    "How do the CCAO’s transparency initiatives aim to redistribute power between the tax lawyer industry, the CCAO, and the constituents of Cook County?\n",
    "\n",
    "**Hint**: You may find the following [official post from CCAO](https://datacatalog.cookcountyil.gov/stories/s/p2kt-hk36) helpful.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Take a look at the Residential Automated Valuation Model files under the Models subgroup in the CCAO’s [GitLab](https://gitlab.com/ccao-data-science---modeling). Without directly looking at any code, do you feel that the documentation sufficiently explains how the residential valuation model works? Which part(s) of the documentation might be difficult for nontechnical audiences to understand?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 4\n",
    "\n",
    "How could the CCAO improve its implementation of transparency? What aspects of it are inaccessible and to whom? Consider the concepts of expertise and power.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### Question 5\n",
    "\n",
    "How does the CCAO's open data initiative support it in maintaining people’s trust in its housing assessments?\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q5\n",
    "manual: true\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e8cbb07a2cc27f7d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "## Part 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('fivethirtyeight')\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)\n",
    "\n",
    "# We will use plot_3d helper function to help us visualize gradients\n",
    "from hw8_utils import plot_3d\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Load Data\n",
    "Load the data.csv file into a pandas dataframe.\n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "load-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "part_1_data = pd.read_csv(\"https://github.com/DS-100/su20/raw/gh-pages/resources/assets/datasets/hw7_data.csv\", index_col=0)\n",
    "part_1_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### A Simple Model\n",
    "\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "First, run the cell below to visualize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(x, y, marker='.')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    \n",
    "x = part_1_data['x']\n",
    "y = part_1_data['y']\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "The data looks roughly linear, with some extra sinusoidal noise. For now, let's assume that the data follows some underlying linear model. We define the underlying linear model that predicts the value $y$ using the value $x$ as: $f_{\\theta^*}(x) = \\theta^* \\cdot x$\n",
    "\n",
    "Since we cannot find the value of the population parameter $\\theta^*$ exactly, we will assume that our dataset approximates our population and use our dataset to estimate $\\theta^*$. We denote an estimate with $\\theta$ and the fitted estimated chosen based on the data as $\\hat{\\theta}$. Our parameterized model is:\n",
    "\n",
    "$$\\Large\n",
    "f_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Based on this equation, we will define the linear model function `linear_model` below to estimate $\\textbf{y}$ (the $y$-values) given $\\textbf{x}$ (the $x$-values) and $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    return theta * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In class, we learned that the squared loss function is smooth and continuous. Let's use squared loss to evaluate our estimate $\\theta$, which we will use later to identify an optimal $\\theta$, denoted $\\hat{\\theta}$. Given observations $y$ and their corresponding predictions $\\hat{y}$, we can compute the average loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def average_squared_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the averge squared loss for observations y and predictions y_hat.\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    return np.mean((y - y_hat) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Finally, we will visualize the average squared loss as a function of $\\theta$, where several different values of $\\theta$ are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- an array containing different estimates of the scalar theta\n",
    "    \"\"\" \n",
    "    avg_loss = np.array([average_squared_loss(linear_model(x, theta), y) for theta in thetas])\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(thetas, avg_loss)\n",
    "    plt.xlabel(\"Theta\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that $\\hat{\\theta}$ is approximately 1.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Fitting our Simple Model\n",
    "\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 6\n",
    "Let's confirm our visual findings for the optimal $\\hat{\\theta}$.\n",
    "\n",
    "Recall from homework 6 that the analytical solution for the optimal $\\hat{\\theta}$ for the average squared loss is: \n",
    "\n",
    "$$\\hat{\\theta} = \\frac{\\sum_{i=1}^n x_i y_i}{\\sum_{i=1}^n x_i^2}$$\n",
    "\n",
    "Now that we have the analytic solution for $\\hat{\\theta}$, implement the function `find_theta` that calculates the numerical value of $\\hat{\\theta}$ based on our data $\\textbf{x}$, $\\textbf{y}$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q6\n",
    "points: 1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "theta_hat_simple = find_theta(x, y)\n",
    "print(f'theta_hat = {theta_hat_simple}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now, let's plot our risk function again using the `visualize` function. But this time, we will add a vertical line at the optimal value of theta (plot the line $\\theta = \\hat{\\theta}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "visualize(x, y, thetas)\n",
    "plt.axvline(x=theta_opt, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We now have an optimal value for $\\theta$ that minimizes the empirical risk. We can use the scatter plot of the data and add the line $f_{\\hat{\\theta}}(x) = \\hat{\\theta} \\cdot \\textbf{x}$ using the $\\hat{\\theta}$ computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt_2 = find_theta(x, y)\n",
    "scatter(x, y)\n",
    "line_values = linear_model(x, theta_opt_2)\n",
    "plt.plot(x, line_values, color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Great! It looks like our estimator $f_{\\hat{\\theta}}(x)$ is able to estimate the average $y$ for each $x$ quite well using a single parameter $\\theta$. \n",
    "\n",
    "The difference between the true $y$'s and the predictions is known as the residual, $\\textbf{r}=\\textbf{y}-\\hat{\\theta} \\cdot \\textbf{x}$. Below, we find the residual and plot the residuals corresponding to $x$ in a scatter plot. We also plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    ...\n",
    "    theta_hat = find_theta(x, y)\n",
    "    y_sin = y - linear_model(x, theta_hat)\n",
    "    plt.scatter(x, y_sin)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('residual (true y - estimated y)')\n",
    "    plt.title('Residual vs x for Linear Model')\n",
    "    plt.axhline(y=0, color='r')\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "part-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Increasing Model Complexity\n",
    "\n",
    "It looks like the residual is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "f_\\boldsymbol\\theta(x) = \\theta_1x + sin(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, which we can represent in the vector, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Note that a general sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. Looking at the residual plot above, it looks like the residual is zero at x = 0, and the residual swings between -1 and 1. Thus, it seems reasonable to effectively set the scaling and phase shifting parameter ($a$ and $c$ in this case) to 1 and 0 respectively. While we could try to fit $a$ and $c$, we're unlikely to get much benefit. When you're done with this assignment, you can try adding $a$ and $c$ to our model and fitting these parameters to see if you can get a better loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "We define the `sin_model` function below that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values) based on our new equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- a vector of length 2, where theta[0] = theta_1 and theta[1] = theta_2\n",
    "    \"\"\"\n",
    "    theta_1 = theta[0]\n",
    "    theta_2 = theta[1]\n",
    "    return theta_1 * x + np.sin(theta_2 * x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 7a\n",
    "\n",
    "Recall the optimal value of $\\theta$ minimizes our loss function. One way of solving for $\\theta$ is by taking the derivative of our loss function with respect to $\\theta$, like we did in HW 6.  \n",
    "\n",
    "Write/derive the expressions for following values and write them with LaTeX in the space below.\n",
    "\n",
    "* $R(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2)$: our loss function, the empirical risk/mean squared error\n",
    "* $\\frac{\\partial R }{\\partial \\theta_1}$: the partial derivative of $R$ with respect to $\\theta_1$\n",
    "* $\\frac{\\partial R }{\\partial \\theta_2}$: the partial derivative of $R$ with respect to $\\theta_2$\n",
    "\n",
    "Recall that $R(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2) = \\frac{1}{n} \\sum_{i=1}^{n} (\\textbf{y}_i - \\hat{\\textbf{y}}_i)^2$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7a\n",
    "manual: True\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "#### Question 7b\n",
    "Now, implement the functions `sin_MSE`, `sin_MSE_dt1` and `sin_MSE_dt2`, which should compute $R$, $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ respectively. Use the expressions you wrote for $\\frac{\\partial R }{\\partial \\theta_1}$ and $\\frac{\\partial R }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$. We have completed `sin_MSE_gradient` for you.\n",
    "\n",
    "Notes: \n",
    "* Keep in mind that we are still working with our original set of data, `part_1_data`.\n",
    "* To keep your code a bit more concise, be aware that `np.mean` does the same thing as `np.sum` divided by the length of the numpy array.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q7b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3c-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_MSE(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the l2 loss of our sinusoidal model given theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "def sin_MSE_dt1(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "def sin_MSE_dt2(theta, x, y):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def sin_MSE_gradient(theta, x, y):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    theta -- the vector of values theta\n",
    "    x     -- the vector of x values\n",
    "    y     -- the vector of y values\n",
    "    \"\"\"\n",
    "    return np.array([sin_MSE_dt1(theta, x, y), sin_MSE_dt2(theta, x, y)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Gradient Descent\n",
    "\n",
    "In lecture, we saw that there are a number of ways to optimize a linear model:\n",
    "\n",
    "1. Brute force guess and check\n",
    "2. Analytically derive a closed form solution (like finding $\\hat{\\theta}$ by taking the derivative of the loss w.r.t. its parameters, as in HW 6)\n",
    "3. Use a numerical method like gradient descent\n",
    "\n",
    "You can try to solve for the optimal $\\hat{\\mathbf{\\theta}}$ analytically using your answers from 2a, but we don't recommend it. Notably, observe that the model is not even linear, e.g. it contains $\\theta_2$ inside a sine function.\n",
    "\n",
    "To demonstrate how truly powerful techniques like gradient descent are, we'll use it in this assignment to optimize our nonlinear model.\n",
    "\n",
    "Let's now implement gradient descent. \n",
    "\n",
    "Note that the function you're implementing here is somewhat different than the gradient descent function we created in lecture. The version in lecture was `gradient_descent(df, initial_guess, alpha, n)`, where `df` was the gradient of the function we are minimizing and `initial_guess` are the starting parameters for that function. Here our signature is a bit different (described below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Question 8a\n",
    "\n",
    "Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in a loss function, the gradient of the loss function, an initial value for $\\theta$ (`theta`), and a dataframe containing the $\\textbf{x}$ and $\\textbf{y}$ values (`data`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate (i.e. the same learning rate at every time step), just like in lecture.\n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the average squared loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Recall that the gradient descent update function follows the form:\n",
    "\n",
    "$$\\Large\n",
    "\\boldsymbol\\theta^{(t+1)} \\leftarrow \\boldsymbol\\theta^{(t)} - \\alpha \\left ( \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_\\boldsymbol\\theta \\mathbf{L}(\\textbf{x}_i, \\textbf{y}_i, \\boldsymbol\\theta^{(t)}) \\right )\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol \\theta$ is the parameter being optimized, $\\alpha$ is the learning rate, $\\nabla_\\boldsymbol\\theta \\mathbf{L}$ is the gradient of the loss function with respect to $\\boldsymbol \\theta$, and $t$ is the optimization step counter (that is, $\\boldsymbol \\theta ^{(t)}$ is the value of $\\boldsymbol \\theta$ after $t$ steps of gradient descent).\n",
    "\n",
    "After completing the function, the cell will output the trajectory from running gradient descent over time.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times. Take a look at `num_iter`.\n",
    "- Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "- Don't forget that `sin_MSE` and `sin_MSE_gradient` require the $x$ and $y$ values to be supplied.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8a\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_theta():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))\n",
    "\n",
    "def grad_desc(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    data -- the data used in the model \n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat, thetas_used, losses_calculated = grad_desc(\n",
    "    sin_MSE, sin_MSE_gradient, theta_start, part_1_data, num_iter=20, alpha=0.1\n",
    ")\n",
    "for b, l in zip(thetas_used, losses_calculated):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 8b\n",
    "Now, let's try using a decaying learning rate. Implement `grad_desc_decay` below, which performs gradient descent with a learning rate that decreases slightly with each time step. You should be able to copy most of your work from the previous part, but you'll need to tweak how you update `theta` at each time step.\n",
    "\n",
    "By decaying learning rate, we mean instead of just a number $\\alpha$, the learning should be now $\\frac{\\alpha}{t+1}$ where $t$ is the number of the current iteration of gradient descent. (Why do we need to add a '+ 1' in the denominator?)\n",
    "\n",
    "**Note:** Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8b\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc_decay(loss_f, gradient_loss_f, theta, data, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and decaying learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    data -- the data used in the model \n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent,\n",
    "                     should include the starting and ending theta (i.e. num_iter + 1 items)\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent, \n",
    "                     should include the starting and ending theta (i.e. num_iter + 1 items)\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat_decay, thetas_used_decay, losses_calculated_decay = grad_desc_decay(\n",
    "    sin_MSE, sin_MSE_gradient, theta_start, part_1_data, num_iter=20, alpha=0.1\n",
    ")\n",
    "for b, l in zip(thetas_used_decay, losses_calculated_decay):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 8c\n",
    "Now, let's try implementing stochastic gradient descent. Implement `stoch_grad_desc` below, which performs stochastic gradient descent. You should be able to copy most of your work from Question 3a. We will be using a static learning rate and a argument `batch_size` to represent the size of the mini-batch to sample for each iteration.\n",
    "\n",
    "Recall that for a mini-batch of the data with size $b$, the stochastic gradient descent update rule is\n",
    "\n",
    "$$\\Large \\theta^{(t+1)} \\leftarrow \\theta^{(t)} - \\alpha \\left ( \\frac{1}{b} \\sum_{i=1}^{b} \\nabla_\\boldsymbol\\theta \\mathbf{L}(\\textbf{x}_i, \\textbf{y}_i, \\boldsymbol\\theta^{(t)}) \\right )$$\n",
    "\n",
    "\n",
    "where $\\mathbf{L}$ is the loss function, and $\\textbf{x}_i$ and $\\textbf{y}_i$ are the $i^\\text{th}$ values in $\\textbf{x}$ and $\\textbf{y}$, respectively.\n",
    "\n",
    "**Note:** In the update rule above, $b$ is much smaller than $n$, the total size of the data.\n",
    "\n",
    "**Note:** Be sure to include the initial theta and loss into the trajectory because the test checks for this.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8c\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def stoch_grad_desc(loss_f, gradient_loss_f, theta, data, batch_size, num_iter=20, alpha=0.1, seed=42):\n",
    "    \"\"\"\n",
    "    Run stochastic gradient descent update for a finite number of iterations\n",
    "\n",
    "    Keyword arguments:\n",
    "    loss_f -- the loss function to be minimized (used for computing loss_history)\n",
    "    gradient_loss_f -- the gradient of the loss function to be minimized\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    data -- the data used in the model \n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent,\n",
    "                     should include the starting and ending theta (i.e. num_iter + 1 items)\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent, \n",
    "                     should include the starting and ending theta (i.e. num_iter + 1 items)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed) # do not change this line\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    ...\n",
    "    return theta, theta_history, loss_history\n",
    "\n",
    "theta_start = init_theta()\n",
    "theta_hat_stoch, thetas_used_stoch, losses_calculated_stoch = stoch_grad_desc(\n",
    "    sin_MSE, sin_MSE_gradient, theta_start, part_1_data, 50, num_iter=20, alpha=0.1\n",
    ")\n",
    "for b, l in zip(thetas_used_stoch, losses_calculated_stoch):\n",
    "    print(f\"theta: {b}, Loss: {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. The code below plots our $x$-values with our model's predicted $\\hat{y}$-values over the original scatter plot. You should notice that gradient descent successfully optimized $\\boldsymbol\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_init = init_theta()\n",
    "\n",
    "theta_est, thetas, loss = grad_desc(sin_MSE, sin_MSE_gradient, theta_init, part_1_data)\n",
    "theta_est_decay, thetas_decay, loss_decay = grad_desc_decay(sin_MSE, sin_MSE_gradient, theta_init, part_1_data)\n",
    "theta_est_stoch, thetas_stoch, loss_stoch = stoch_grad_desc(sin_MSE, sin_MSE_gradient, theta_init, part_1_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = sin_model(x, theta_est)\n",
    "\n",
    "plt.plot(x, y_pred, label='Model ($\\hat{y}$)')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation ($y$)', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Run the following cell to see a plot of the loss values over each iteration of the following 3 variations of gradient descent: gradient descent with a static learning rate, gradient descent with a decaying learning rate, and stochastic gradient descent with a static learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(loss)), loss, label='Static Alpha')\n",
    "plt.plot(np.arange(len(loss)), loss_decay, label='Decaying Alpha')\n",
    "plt.plot(np.arange(len(loss)), loss_stoch, label='SGD')\n",
    "plt.xlabel('Iteration #')\n",
    "plt.ylabel('Avg Loss')\n",
    "plt.title('Avg Loss vs Iteration # vs Learning Rate Type')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Visualizing Loss\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent optimizes our model parameters.\n",
    "\n",
    "In the previous plot we saw the loss decrease with each iteration. In this part, we'll see the trajectory of the algorithm as it travels the loss surface? Run the following cells to see visualization of this trajectory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "thetas = np.array(thetas).squeeze()\n",
    "thetas_decay = np.array(thetas_decay).squeeze()\n",
    "thetas_stoch = np.array(thetas_stoch).squeeze()\n",
    "loss = np.array(loss)\n",
    "loss_decay = np.array(loss_decay)\n",
    "loss_stoch = np.array(loss_stoch)\n",
    "thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "plot_3d(thetas[:, 0], thetas[:, 1], loss, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loss-3d-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (gradient descent with decaying alpha)\n",
    "plot_3d(thetas_decay[:, 0], thetas_decay[:, 1], loss_decay, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (stochastic gradient descent)\n",
    "plot_3d(thetas_stoch[:, 0], thetas_stoch[:, 1], loss_stoch, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Another common way of visualizing 3D dynamics is with a _contour_ plot. Run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    theta1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    theta2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(theta1_s, theta2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for theta1, theta2 in data:\n",
    "        l = loss_function(model(x, np.array([theta1, theta2])), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create trace of theta point\n",
    "    # Create the contour \n",
    "    theta_points = go.Scatter(name=\"theta Values\", \n",
    "                              x=theta_1_series, \n",
    "                              y=theta_2_series,\n",
    "                              mode=\"lines+markers\")\n",
    "    lr_loss_contours = go.Contour(x=theta1_s, \n",
    "                                  y=theta2_s, \n",
    "                                  z=z, \n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, theta_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Static Learning Rate', thetas, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q5b-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "contour_plot('Gradient Descent with Decay Learning Rate', thetas_decay, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot('Stochastic Gradient Descent', thetas_stoch, average_squared_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "#### Question 8d: Analyzing Learning Rates\n",
    "\n",
    "In 1-2 sentences, describe what you notice about the path that $\\theta$ takes with a static learning rate vs. a decaying learning rate and for batch vs. stochastic gradient descent. Based on these comparisons, why do we often prefer SGD over batch GD in practice? In your answer, refer to the plots above.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q8d\n",
    "manual: true\n",
    "points: 2\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "**Congrats, you've finished Homework 8!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
