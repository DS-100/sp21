---
layout: page
title: Lecture 12 – Simple Linear Regression
nav_exclude: true
---

# Lecture 12 – Simple Linear Regression

Presented by Andrew Bray and Suraj Rampure

Content by Andrew Bray, Suraj Rampure, and Ani Adhikari

- [12.1 slides](../../resources/assets/lectures/lec12/lec-12-1.html) 
- [12.2-12.6 slides](https://docs.google.com/presentation/d/1rHflVaF3aTMD7h96Bv0cwTkbrCTo5xNVq_dfOL7dpCE/edit?usp=sharing)
- [video playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfrDgGa4I0l1ciw39lmYpmGg)
- [code](https://github.com/DS-100/sp21/tree/main/lec/lec12) ([launch](https://data100.datahub.berkeley.edu/hub/user-redirect/git-sync?repo=https://github.com/DS-100/sp21&subPath=lec/lec12/&branch=main))
- [code HTML](../../resources/assets/lectures/lec12/lec12.html)

A reminder – the right column of the table below contains _Quick Checks_. These are **not** required but suggested to help you check your understanding.

<table>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>Video</th>
<th>Quick Check</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>12.1</strong> <br>Introduction the linear model.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/xvJxGmuT7LY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td></td>
</tr>
<tr>
<td><strong>12.2</strong> <br>Using calculus to derive the optimal model parameters for the simple linear regression model, when we choose squared loss as our loss function.</td>
<td><iframe width="300" height="500" height src="https://youtube.com/embed/7hVK78Ir618" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSeY0yhl9hQRZmxlMe42kvE5pbGJHwyjoTOp-BPJZNiRJTt-Cg/viewform" target="\_blank">12.2</a></td>
</tr>
<tr>
<td><strong>12.3</strong> <br>Visualizing and interpreting loss surface of the SLR model.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/K3e19T_Z9JU" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSemBu1vJ5j_uoPzLqSGCDh-4GBGRf-M4mW-vXjweMMlAtGrpQ/viewform" target="\_blank">12.3</a></td>
</tr>
<tr>
<td><strong>12.4</strong> <br>Interpreting the slope of the simple linear model. </td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/dKI_lDXDzvI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfNAMYYJxNWFJlpz3Wl7Xa2jEMalnBw6FLsjr2ukcjDZ1XM_g/viewform" target="\_blank">12.4</a></td>
</tr>
<tr>
<td><strong>12.5</strong> <br>Defining key terminology in the regression context. Expanding the simple linear model to include any number of features.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/LHbuY63Bh_0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLSfb77dDmoZlrkdQH3ocSFw1xr-_YQdYg1hRZv2oUrxUcaf8Lw/viewform" target="\_blank">12.5</a></td>
</tr>
<tr>
<td><strong>12.6</strong> <br>RMSE as a metric of accuracy. Multiple R-squared as a metric of explained variation. Summary.</td>
<td><iframe width="300" height="300" height src="https://youtube.com/embed/1jLglngUYUM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><a href="https://docs.google.com/forms/d/e/1FAIpQLScZ40zJtTzCPhjZf-V4op0lgujVp__HNlAIKKkHi95aYq_Ejg/viewform" target="\_blank">12.6</a></td>
</tr>
