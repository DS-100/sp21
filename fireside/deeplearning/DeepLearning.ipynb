{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Introduction to Deep Learning\n",
    "\n",
    "In this notebook, we provide a very quick (shallow?) introduction to neural networks and deep learning.  We review the basic challenge of binary classification and linear decision functions and then show how features can be composed to express more complex decision surfaces.  We then build a basic neural network to learn the feature functions and ultimately build more complex models for image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Review of Logistic Regression\n",
    "\n",
    "We start by reviewing logistic regression.  We construct a linearly separable data set and show how a logistic regression model fits this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(4*n, 2) + 3*np.tile([[1,1], [-1,1], [-1,-1], [1,-1]],(n, 1))\n",
    "y = x[:,0]>0\n",
    "data = pd.DataFrame(np.hstack([x,y[:,np.newaxis]]), columns=[\"X1\", \"X2\", \"Y\"]).sample(frac=1)\n",
    "pos_ind = data[\"Y\"]==1.0\n",
    "pos_scatter = go.Scatter(x=data.loc[pos_ind,\"X1\"], y=data.loc[pos_ind,\"X2\"],\n",
    "                         mode=\"markers\", marker_symbol=\"cross\", name=\"Pos\")\n",
    "neg_scatter = go.Scatter(x=data.loc[~pos_ind,\"X1\"], y=data.loc[~pos_ind,\"X2\"], \n",
    "                         mode=\"markers\", name=\"Neg\")\n",
    "go.Figure([pos_scatter, neg_scatter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(data[[\"X1\", \"X2\"]], data['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code generates the prediction surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(predict_fn):\n",
    "    u = np.linspace(-10, 10, 100)\n",
    "    (x0, x1) = np.meshgrid(u,u)\n",
    "    X = np.vstack([x0.flatten(), x1.flatten()]).T\n",
    "    Y_hat = predict_fn(X)\n",
    "    return go.Contour(x=X[:,0], y=X[:,1], z=Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure([pos_scatter, neg_scatter, \n",
    "           plot_predictions(lambda X: model.predict_proba(X)[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the above plot we assign near zero probability of being a \"plus\" to the region on the left and a near one probability to the region on the right.  Also notice that in the middle there is a transition region as the probability being a \"plus\" goes from zero to one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-linearly Separable Data\n",
    "\n",
    "We can modify the above data slightly to construct a data set that is no longer linearly separable.  Can you find a decision line that would separate this data into \"plus\" and \"circle\" regions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(4*n, 2) + 3*np.tile([[1,1], [-1,1], [-1,-1], [1,-1]],(n, 1))\n",
    "y = np.logical_xor(x[:,0]>0, x[:,1]>0)\n",
    "data = pd.DataFrame(np.hstack([x,y[:,np.newaxis]]), columns=[\"X1\", \"X2\", \"Y\"]).sample(frac=1)\n",
    "pos_ind = data[\"Y\"]==1.0\n",
    "pos_scatter = go.Scatter(x=data.loc[pos_ind,\"X1\"], y=data.loc[pos_ind,\"X2\"],\n",
    "                         mode=\"markers\", marker_symbol=\"cross\", name=\"Pos\")\n",
    "neg_scatter = go.Scatter(x=data.loc[~pos_ind,\"X1\"], y=data.loc[~pos_ind,\"X2\"], \n",
    "                         mode=\"markers\", name=\"Neg\")\n",
    "go.Figure([pos_scatter, neg_scatter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we fit a logistic regression classifier to this data we no longer get an effective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(data[[\"X1\", \"X2\"]], data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure([pos_scatter, neg_scatter, \n",
    "           plot_predictions(lambda X: model.predict_proba(X)[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How could we improve the classifier performance?  One standard solution would be to leverage feature engineering.   What feature transformations would help with this classification problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Feature Engineering\n",
    "\n",
    "Looking at the above figure it seems like the class depends on which quadrant the point is drawn from.  We could one-hot encode the quadrant for each point a fit a model using these features instead of the original $X_1$ and $X_2$ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quadrant_features(X):\n",
    "    df = pd.DataFrame()\n",
    "    df[\"quad1\"] = (X[:, 0] > 0) & (X[:, 1] > 0)\n",
    "    df[\"quad2\"] = (X[:, 0] < 0) & (X[:, 1] > 0)\n",
    "    df[\"quad3\"] = (X[:, 0] < 0) & (X[:, 1] < 0)\n",
    "    df[\"quad4\"] = (X[:, 0] > 0) & (X[:, 1] < 0)\n",
    "    return df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again fitting the logistic regression model with these 4 new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(quadrant_features(data[[\"X1\", \"X2\"]].to_numpy()), data['Y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we plot the decision surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure([pos_scatter, neg_scatter, \n",
    "           plot_predictions(lambda X: model.predict_proba(quadrant_features(X))[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we are able to accurately classify our data. The advantage of this approach is we are able to use domain knowledge and intuition in the design of our model.  However, for many real-world problems, it may be very difficult to manually create these kinds of highly informative features.  We would like to learn the features themselves.  Notice that in the above example the features were also binary functions (just like the logistic regression function).  Could we use logistic regression to build features as well as the final classifier? This is where neural networks begin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "Classically, the standard approach to building models is to leverage domain knowledge to engineer features that capture the concepts we are trying to model.  For example, if we want to detect cats in images we might want to look for edges, texture, and geometry that are unique to cats.  These features are then fed into high-dimensional robust classification models like logistic regression. \n",
    "\n",
    "<center>\n",
    "<img src=\"feature_engineering_pipelines.png\" alt=\"Classic Feature Engineering Pipeline\" width=\"100%\">\n",
    "</center>\n",
    "\n",
    "Descriptive features (e.g., cat textures) are often used as inputs to increasingly higher level features (e.g, cat ears).  This composition of features results in \"deep\" pipelines of transformations producing increasingly more abstract feature concepts. However, manually designing these features is both challenging and may not actually produce the optimal feature representations. \n",
    "\n",
    "The idea in Deep Learning is to automatically learn entire pipelines of feature transformations and the resulting classifier from data.  This is accomplished use neural networks.  While neural networks were originally inspired by _abstract_ models of neural computation, modern neural networks can be more accurately characterized as complex parameteric function expressed programatically as the composition of mathematical primitives.   In the following, we will first describe a simple neural network pictorially and then programatically. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Basic Neuron\n",
    "\n",
    "Neural networks originated from a simple mathematical abstraction of a \"neuron\" as a computational device that accumulates input signals and produces an output.  The following is a very simple diagram of a neuron.  Conceptually signal arrive at the dendrites on the left and when the combine firing is sufficient to trigger an action potential across the axon an output is sent to the axon terminals on the right.  \n",
    "\n",
    "<center>\n",
    "<img src=\"neuron.png\" alt=\"Simple Neuron\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "We can model this process (very abstractly) as a weighted summation of input values that is then transmitted to the output when the inputs exceed some threshold.  This activation threshold could be modeled by a sigmoid function like is used in logistic regression.  In this case, the behavior of this single artificial neuron is precisely the logistic regression model.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"artificial_neuron.png\" alt=\"Artificial Neuron\" width=\"40%\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine these single artificial neurons into larger networks of neurons to express more complex functions.  For example, the following network has multiple layers of _neurons_:\n",
    "\n",
    "<center>\n",
    "<img src=\"two_hidden_layers.png\" alt=\"Artificial Neuron\" width=\"40%\">\n",
    "</center>\n",
    "\n",
    "This can be written as a mathematical expression:\n",
    "\n",
    "\\begin{align}\n",
    "A_1 & = \\text{Sigmoid}(W X) \\\\\n",
    "A_2 & = \\text{Sigmoid}(U A_1) \\\\\n",
    "\\mathbb{P}(Y=1 \\,|\\, X) & = \\text{Sigmoid}(V A_2)\n",
    "\\end{align}\n",
    "\n",
    "Here we assume that there is an implicit _bias_ term added to each stage and the initial layer weight matrix $W\\in \\mathbb{R}^{4,3}$, the next layer weight matrix is in $U \\in \\mathbb{R}^{5,3}$, and the final weight matrix is in $V \\in \\mathbb{R}^{4,1}$.  The vectors (often called _activations_) $A_1 \\in \\mathbb{R}^4$ and $A_1 \\in \\mathbb{R}^3$ correspond to the learned intermediate features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "\n",
    "One of the significant innovations in deep learning is the introduction of libraries to simplify the design and training of neural networks.  These libraries allow users to easily describe complex network structures and then automatically derivate optimization procedures to train these networks.\n",
    "\n",
    "In the following we will use PyTorch to implement such a network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a lot like Numpy in that you can express interesting computation interms of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = torch.from_numpy(data[[\"X1\", \"X2\"]].to_numpy())\n",
    "tY = torch.from_numpy(data[\"Y\"].to_numpy()).long()\n",
    "dataset = TensorDataset(tX, tY)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(model, loss_fn, dataset, l2reg = 1e-5, lr=1.0, nepochs=200, batch_size=10):\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    opt = Adam(model.parameters(), lr=lr, weight_decay=l2reg)\n",
    "    for i in range(nepochs):\n",
    "        for (x, y) in loader:\n",
    "            loss = loss_fn(model(x), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicNN = nn.Sequential(\n",
    "    nn.Linear(2,4), nn.Sigmoid(), nn.Linear(4,2) \n",
    ").double()\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "stochastic_gradient_descent(basicNN, loss_fn, dataset, l2reg=1e-4, lr=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_predict(model, npX):\n",
    "    with torch.no_grad():\n",
    "        return F.softmax(model.forward(torch.from_numpy(npX)), dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure([pos_scatter, neg_scatter, \n",
    "           plot_predictions(lambda X: softmax_predict(basicNN, X)[:,1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Introduction to Algorithmic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we are going to introduce PyTorch.  PyTorch is sort of like learning how to use Thor's hammer, it is way overkill for basically everything you will do and is probably the wrong solution to most problems you will encounter. However, it also really powerful and will give you the skills needed to take on very challenging problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a variable $\\theta$ with an initial value 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = torch.tensor([1.0], requires_grad=True, dtype=torch.float64)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we compute the following value from our tensor `theta`\n",
    "\n",
    "$$\n",
    "z = \\left(1 - log\\left(1 + \\exp(\\theta) \\right) \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (1 - torch.log(1 + torch.exp(theta)))**2\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that every derived value has an attached a gradient function that is used to compute the backwards path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.grad_fn.next_functions[0][0].next_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize these functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchviz\n",
    "# !brew install graphviz\n",
    "from torchviz import make_dot\n",
    "make_dot(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These backward functions tell Torch how to compute the gradient via the chain rule.  This is done by invoking backward on the computed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `item` to extract a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta.grad.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this witht he hand computed derivative:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial z}{\\partial\\theta} &= \\frac{\\partial}{\\partial\\theta}\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)^2 \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)\\frac{\\partial}{\\partial\\theta} \\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)\\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right) (-1) \\frac{\\partial}{\\partial\\theta} \\log\\left(1 + \\exp(\\theta)\\right) \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{-1}{1 + \\exp(\\theta)}\\frac{\\partial}{\\partial\\theta}\\left(1 + \\exp(\\theta)\\right) \\\\\n",
    " & = 2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{-1}{1 + \\exp(\\theta)}\\exp(\\theta) \\\\\n",
    "  & = -2\\left(1 - \\log\\left(1 + \\exp(\\theta)\\right)\\right)   \\frac{\\exp(\\theta)}{1 + \\exp(\\theta)}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_derivative(theta):\n",
    "    return -2 * (1 - np.log(1 + np.exp(theta))) * np.exp(theta) / (1. + np.exp(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_derivative(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionCNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(FashionCNN, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64*6*6, out_features=600)\n",
    "        self.drop = nn.Dropout2d(0.25)\n",
    "        self.fc2 = nn.Linear(in_features=600, out_features=120)\n",
    "        self.fc3 = nn.Linear(in_features=120, out_features=10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.fc2(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FashionCNN()\n",
    "model.to(device)\n",
    "\n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    model.train()\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 2\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_images = torch.Tensor()\n",
    "error_labels = torch.Tensor()\n",
    "error_pred = torch.Tensor()\n",
    "model.eval()    \n",
    "with torch.no_grad():\n",
    "    for X, y in test_dataloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X).argmax(1)\n",
    "        errors = pred != y\n",
    "        error_images = torch.cat([error_images, X[errors,:,:,:]])\n",
    "        error_labels = torch.cat([error_labels, y[errors]])\n",
    "        error_pred = torch.cat([error_pred, pred[errors]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(error_labels), size=(1,)).item()\n",
    "    img = error_images[sample_idx]\n",
    "    label = error_labels[sample_idx].item()\n",
    "    pred = error_pred[sample_idx].item()\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label] +\", pred=\" + labels_map[pred])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
