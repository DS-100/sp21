{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Values, Categorical Features, and Text\n",
    "\n",
    "\n",
    "In this notebook, we discuss:\n",
    "1. how to deal with missing values\n",
    "2. how to encode categorical features,\n",
    "3. and how to encode text features.\n",
    "\n",
    "In the process, we will work through feature engineering to construct a model that predicts vehicle efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "For this notebook, we will use the seaborn `mpg` data set which describes the fuel mileage (measured in miles per gallon or mpg) of various cars along with characteristics of those cars.  Our goal will be to build a model that can predict the fuel mileage of a car based on the characteristics of that car."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import load_dataset\n",
    "data = load_dataset(\"mpg\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Continuous Features\n",
    "\n",
    "This data set has several quantitative continuous features that we can use to build our first model.  However, even for quantitative continuous features, we may want to do some additional feature engineering.  Things to consider are:\n",
    "\n",
    "1. transforming features with non-linear functions (log, exp, sine, polynomials)\n",
    "2. constructing products or ratios of features\n",
    "3. dealing with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "We can use the Pandas `DataFrame.isna` function to find rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to deal with missing values.  A common strategy is to substitute the mean.  Because missing values can actually be useful signal, it is often a good idea to include a feature indicating that the value was missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_cont(df):\n",
    "    Phi = df[[\"cylinders\", \"displacement\", \n",
    "              \"horsepower\", \"weight\", \n",
    "              \"acceleration\", \n",
    "              \"model_year\"]].copy()\n",
    "    Phi[\"horsepower_missing\"] = Phi[\"horsepower\"].isna()\n",
    "    Phi = Phi.fillna(Phi.mean())\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our feature function, we can fit our first model to the transformed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(phi_cont(data), data[[\"mpg\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping Track of Progress\n",
    "\n",
    "Because we are going to be building multiple models with different feature functions it is important to have a standard way to track each of the models.  \n",
    "\n",
    "The following function takes a model prediction function, the name of a model, and the dictionary of models that we have already constructed.  It then evaluates the new model on the data and plots how the new model performs relative to the previous models as well as the $Y$ vs $\\hat{Y}$ scatter plot.  \n",
    "\n",
    "In addition, it updates the dictionary of models to include the new model for future plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, phi, models=dict()):\n",
    "    # run the prediction function and compute the RMSE\n",
    "    Yhat = model.predict(phi(data)).flatten()\n",
    "    Y = data['mpg'].to_numpy()\n",
    "    rmse = np.sqrt(mean_squared_error(Y, Yhat))\n",
    "    print(\"Root Mean Squared Error:\", rmse)\n",
    "    \n",
    "    # Save the model and rmse to the collection of models \n",
    "    models[name] = dict(model=model, phi=phi, rmse=rmse)\n",
    "    \n",
    "    # Generate diagnostic and model comparison plots\n",
    "    fig = make_subplots(rows=1, cols=2)\n",
    "    fig.add_trace(go.Scatter(x=Yhat, y=Y, mode=\"markers\"), row=1, col=1)\n",
    "    fig.update_xaxes(title = \"Yhat\", row=1, col=1)\n",
    "    fig.update_yaxes(title = \"Y\", row=1, col=1)\n",
    "    ymin = np.min(Yhat)\n",
    "    ymax = np.max(Yhat)\n",
    "    fig.add_trace(go.Scatter(x=[ymin,ymax], y=[ymin,ymax], name=\"y=yhat\"), row=1, col=1)\n",
    "    fig.add_trace(go.Bar(x=list(models.keys()), \n",
    "                         y=[models[k]['rmse'] for k in models]), row=1, col=2)    \n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.update_yaxes(title = \"RMSE\", row=1, col=2)\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(\"cont.\", model, phi_cont, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stable Feature Functions\n",
    "\n",
    "Unfortunately, the feature function we just implemented applies a different transformation depending on what input we provide. Specifically, if the `horesepower` is missing when we go to make a prediction we will substitute it with a different mean then was used when we fit our model.  Furthermore, if we only want predictions on a few records and the `horsepower` is missing from those records then the feature function will be unable to substitute a meaningful value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if we were to get new records that look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data[data['horsepower'].isna()].head(3)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature function is be unable to substitute the mean since none of the records have a `horsepower` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.predict(phi_cont(new_data))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix this by computing the mean on the original data and using that mean on any new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a global variable\n",
    "def phi_cont(df, data_mean = data.mean()):\n",
    "    feature_cols = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "    Phi = df[feature_cols].copy()\n",
    "    Phi[\"horsepower_missing\"] = Phi[\"horsepower\"].isna().astype(float)\n",
    "    Phi = Phi.fillna(data_mean)\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(phi_cont(new_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn Model Imputer\n",
    "\n",
    "Because these kinds of transformations are fairly common. Scikit-learn has built-in transformations for data imputation.  These transformations have a common pattern of `fit` and `transform`.  You first `fit` the transformation to your data and then you can `transform` your data and any future data using the same transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(data[['weight', 'horsepower']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.transform(data[['weight', 'horsepower']])[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.fit(data[['horsepower']])\n",
    "def phi_cont(df, imputer=imputer):\n",
    "    feature_cols = [\"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\"]\n",
    "    Phi = df[feature_cols].copy()\n",
    "    Phi[\"horsepower_missing\"] = Phi[\"horsepower\"].isna().astype(float)\n",
    "    Phi[\"horsepower\"] = imputer.transform(Phi[[\"horsepower\"]]).flatten()\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(phi_cont(data), data[[\"mpg\"]])\n",
    "evaluate_model(\"cont.\", model, phi_cont, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Domain Knowledge\n",
    "\n",
    "The displacement of an engine is defined as the product of the volume of each cylinder and number of cylinders.  However, not all cylinders fire at the same time (at least in a functioning engine) so the fuel economy might be more closely related to the volume of any one cylinder.  \n",
    "\n",
    "![Cylinders from https://gifimage.net/piston-gif-3/](https://gifimage.net/wp-content/uploads/2018/04/piston-gif-3.gif)\n",
    "\n",
    "We can use this \"domain knowledge\" to compute a new feature encoding the volume per cylinder by taking the ratio of displacement and cylinders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_with_displacement(df):\n",
    "    Phi = phi_cont(df)\n",
    "    Phi['displacement/cylinder'] = Phi['displacement'] / Phi['cylinders']\n",
    "    return Phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_with_displacement(data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again fitting and evaluating our model we see a reduction in prediction error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(phi_with_displacement(data), data[[\"mpg\"]])\n",
    "evaluate_model(\"cont.+(d/c)\", model, phi_with_displacement, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding Categorical Data \n",
    "\n",
    "The `origin` column in this data set is categorical (nominal) data taking on a fixed set of possible values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(data, x='origin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this kind of data in a model, we need to transform into a vector encoding that treats each distinct value as a separate dimension.  This is called One-hot Encoding or Dummy Encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding (Dummy Encoding)\n",
    "\n",
    "\n",
    "One-Hot encoding, sometimes also called **dummy encoding** is a simple mechanism to encode categorical data as real numbers such that the magnitude of each dimension is meaningful.  Suppose a feature can take on $k$ distinct values (e.g., $k=50$ for 50 states in the United Stated).  A new feature (dimension) is created for each distinct value.  For each record, all the new features are set to zero except the one corresponding to the value in the original feature. \n",
    "\n",
    "<img src=\"images/one_hot_state.png\" width=\"600px\">\n",
    "\n",
    "The term one-hot encoding comes from a digital circuit encoding of a categorical state as particular \"hot\" wire:\n",
    "\n",
    "<img src=\"images/one_hot_encoding.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy Encoding in Pandas\n",
    "\n",
    "We can construct a one-hot (dummy) encoding of the origin column using the `Pandas.get_dummies` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(data[['origin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `Pandas.get_dummies`, we can build a new feature function which extends our previous features with the additional dummy encoding columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_with_origin(df):\n",
    "    Phi = phi_with_displacement(df)\n",
    "    return Phi.join(pd.get_dummies(df[['origin']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_with_origin(data).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a new model with the origin feature encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(phi_with_origin(data), data[[\"mpg\"]])\n",
    "evaluate_model(\"cont.+(d/c)+o\", model, phi_with_origin, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the above feature function is not stable.  For example, if we are given a single vehicle to make a prediction the model will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.predict(phi_with_origin(data.head(1)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see why this fails look at the feature transformation for a single row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_with_origin(data.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dummy columns are not created for the other categories.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple solutions.  We could maintain a list of dummy columns and always add these columns.  Alternatively, we could use a library function designed to solve this problem.  The second option is much easier.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn One-hot Encoder\n",
    "\n",
    "The scikit-learn library has a wide range feature transformations and a framework for composing them in reusable (stable) pipelines.  Let's first look at a basic [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "oh_enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit that instance to some data.  This is where we would determine the specific values that a categorical feature can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_enc.fit(data[['origin']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we fit the transformation, we can then use it transform new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_enc.transform(data[['origin']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_enc.transform(data[['origin']].head()).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also inspect the categories of the one-hot encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_enc.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update our feature function to use the one-hot encoder instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_with_origin(df):\n",
    "    Phi = phi_with_displacement(df)\n",
    "    dummies = pd.DataFrame(oh_enc.transform(df[['origin']]).todense(), \n",
    "                           columns=oh_enc.get_feature_names(),\n",
    "                           index = df.index)\n",
    "    return Phi.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_with_origin(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(phi_with_origin(data), data[[\"mpg\"]])\n",
    "evaluate_model(\"cont.+(d/c)+o\", model, phi_with_origin, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding _Text_ Features\n",
    "\n",
    "The only remaining feature to encode is the vehicle name.  Is there potentially signal in the vehicle name?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['name']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding text can be challenging.  The capturing the semantics and grammar of language in mathematical (vector) representations is an active area of research.  State-of-the-art techniques often rely on neural networks trained on large collections of text. In this class, we will focus on basic text encoding techniques that are still widely used.  If you are interested in learning more, checkout [BERT Explained: A Complete Guide with Theory and Tutorial](https://towardsml.com/2019/09/17/bert-explained-a-complete-guide-with-theory-and-tutorial).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here we present two widely used representations of text:\n",
    "\n",
    "* **Bag-of-Words Encoding**: encodes text by the frequency of each word\n",
    "* **N-Gram Encoding**: encodes text by the frequency of sequences of words of length $N$\n",
    "\n",
    "Both of these encoding strategies are related to the one-hot encoding with dummy features created for every word or sequence of words and with multiple dummy features having counts greater than zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bag-of-Words Encoding\n",
    "\n",
    "\n",
    "The bag-of-words encoding is widely used and a standard representation for text in many of the popular text clustering algorithms.  The following is a simple illustration of the bag-of-words encoding:\n",
    "\n",
    "<img src=\"images/bag_of_words.png\" width=\"600px\">\n",
    "\n",
    "**Notice**\n",
    "1. **Stop words are often removed.** Stop-words are words like `is` and `about` that in isolation contain very little information about the meaning of the sentence.  Here is a good list of [stop-words in many languages](https://code.google.com/archive/p/stop-words/). \n",
    "1. **Word order information is lost.**  Nonetheless the vector still suggests that the sentence is about `fun`, `machines`, and `learning`.  Thought there are many possible meanings _learning machines have fun learning_ or _learning about machines is fun learning_ ...\n",
    "1. **Capitalization and punctuation are typically removed.**  However, emoji symbols may be worth preserving.  \n",
    "1. **Sparse Encoding:** is necessary to represent the bag-of-words efficiently.  There are millions of possible words (including terminology, names, and misspellings) and so instantiating a `0` for every word that is not in each record would be inefficient.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Professor Gonzalez is an \"artist\"\n",
    "\n",
    "When professor Gonzalez was a graduate student at Carnegie Mellon University, he and several other computer scientists created the following art piece on display in the Gates Center:\n",
    "\n",
    "<img src=\"images/bag_of_words_art.jpg\" width=\"300px\">\n",
    "\n",
    "Is this art or science? \n",
    "\n",
    "**Notice**\n",
    "1. The unordered collection of words in the bag.\n",
    "1. The stop words on the floor.\n",
    "1. _The missing broom._  The original sculpture had a broom attached but the janitor got confused .... \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words in Scikit-learn\n",
    "\n",
    "We can use scikit-learn to construct a bag-of-words representation of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frost_text = [x for x in \"\"\"\n",
    "Some say the world will end in fire,\n",
    "Some say in ice.\n",
    "From what Ive tasted of desire\n",
    "I hold with those who favor fire.\n",
    "\"\"\".split(\"\\n\") if len(x) > 0]\n",
    "\n",
    "frost_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Construct the tokenizer with English stop words\n",
    "bow = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# fit the model to the passage\n",
    "bow.fit(frost_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the words that are kept\n",
    "print(\"Words:\", list(enumerate(bow.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sentence Encoding: \\n\")\n",
    "# Print the encoding of each line\n",
    "for (text, encoding) in zip(frost_text, bow.transform(frost_text)):\n",
    "    print(text)\n",
    "    print(encoding.todense())\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The N-Gram Encoding\n",
    "\n",
    "The n-gram encoding is a generalization of the bag-of-words encoding designed to capture information about word ordering.  Consider the following passage of text:\n",
    "\n",
    "> _The book was not well written but I did enjoy it._\n",
    "\n",
    "If we re-arrange the words we can also write:\n",
    "\n",
    "> _The book was well written but I did not enjoy it._\n",
    "\n",
    "Moreover, local word order can be important when making decisions about text.  The n-gram encoding captures local word order by defining counts over sliding windows. In the following example a bi-gram ($n=2$) encoding is constructed:\n",
    "\n",
    "<img src=\"images/ngram.png\" width=\"800px\">\n",
    "\n",
    "The above n-gram would be encoded in the sparse vector:\n",
    "\n",
    "<img src=\"images/ngram_vector.png\" width=\"300px\">\n",
    "\n",
    "Notice that the n-gram captures key pieces of sentiment information: `\"well written\"` and `\"not enjoy\"`.  \n",
    "\n",
    "N-grams are often used for other types of sequence data beyond text. For example, n-grams can be used to encode genomic data, protein sequences, and click logs. \n",
    "\n",
    "**N-Gram Issues**\n",
    "1. Maintaining the dictionary of possible n-grams can be very costly.  There are several approximations leveraging hashing that can be used to closely approximate n-gram encoding without the need to maintain the dictionary of all possible n-grams. \n",
    "1. As the size $n$ of n-grams increases the chance of observing more than one instance decreases limiting their value as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the tokenizer with English stop words\n",
    "bigram = CountVectorizer(ngram_range=(1, 2))\n",
    "# fit the model to the passage\n",
    "bigram.fit(frost_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the words that are kept\n",
    "print(\"\\nWords:\", \n",
    "      list(zip(range(0,len(bigram.get_feature_names())), bigram.get_feature_names())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSentence Encoding: \\n\")\n",
    "# Print the encoding of each line\n",
    "for (text, encoding) in zip(frost_text, bigram.transform(frost_text)):\n",
    "    print(text)\n",
    "    print(encoding.todense())\n",
    "    print(\"------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Text Encoding\n",
    "\n",
    "We can add the text encoding features to our feature function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer()\n",
    "bow.fit(data[\"name\"])\n",
    "\n",
    "def phi_with_name(df):\n",
    "    Phi = phi_with_origin(df)\n",
    "    bow_encoding = pd.DataFrame(\n",
    "        bow.transform(df['name']).todense(), \n",
    "        columns=bow.get_feature_names(),\n",
    "        index = df.index)\n",
    "    return Phi.join(bow_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Phi = phi_with_name(data)\n",
    "Phi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(phi_with_name(data), data[[\"mpg\"]])\n",
    "evaluate_model(\"cont.+(d/c)+o+n\", model, phi_with_name, models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Reflection\n",
    "\n",
    "Notice that as we added more features we were able to improve the accuracy of our model.  This is not always a good thing and we will see the problems associated with this in a future lecture.  \n",
    "\n",
    "It is also worth noting that our feature functions each depended on the last and the in some cases we were converting sparse features to dense features.  There is a better way to deal with feature pipelines using the scikit-learn pipelines module.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
